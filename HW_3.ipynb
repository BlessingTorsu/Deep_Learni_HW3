{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNhwOUwzbzGFK9qrAUxSBfj"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"widgets":{"application/vnd.jupyter.widget-state+json":{"30039e9b48ab4e10b0a273ae3c96ba84":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_491e74054e2b41c6bb399b5f75ddbbc9","IPY_MODEL_cec7477a281d43a4a451fe8df6da120b","IPY_MODEL_f33e26922bd1411ebc215c6653672e81"],"layout":"IPY_MODEL_429ad9a3a0634802b4107eb0fd4c8dfe"}},"491e74054e2b41c6bb399b5f75ddbbc9":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_25a80cd93e844819920ceb85189fbecb","placeholder":"​","style":"IPY_MODEL_629362288d9b41d8846ab3f4162914b9","value":"tokenizer_config.json: 100%"}},"cec7477a281d43a4a451fe8df6da120b":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_3b2a72e0838f4aa4a9907baa1c96b36a","max":48,"min":0,"orientation":"horizontal","style":"IPY_MODEL_b45b3c330201474aac99b07cfdc021ea","value":48}},"f33e26922bd1411ebc215c6653672e81":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_ec29a4b98bca47a5aecd49a0bb879af2","placeholder":"​","style":"IPY_MODEL_f68de5c27e5646289fa224c896892b2d","value":" 48.0/48.0 [00:00&lt;00:00, 1.96kB/s]"}},"429ad9a3a0634802b4107eb0fd4c8dfe":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"25a80cd93e844819920ceb85189fbecb":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"629362288d9b41d8846ab3f4162914b9":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"3b2a72e0838f4aa4a9907baa1c96b36a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b45b3c330201474aac99b07cfdc021ea":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"ec29a4b98bca47a5aecd49a0bb879af2":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f68de5c27e5646289fa224c896892b2d":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"6741d53566be4086ac2ba85dbb979952":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_fad2e9ceef184cec89de85b70921f838","IPY_MODEL_01852e9688d142dd8b48709756057d1e","IPY_MODEL_b5ae8de97fbd4c7d930baec3b1747931"],"layout":"IPY_MODEL_85b52838d1c54bfeaca883dc95da1466"}},"fad2e9ceef184cec89de85b70921f838":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_676963610ce349039c32eac82cd0d86a","placeholder":"​","style":"IPY_MODEL_b39c15429cbb497bb09d0eb2e5101c52","value":"vocab.txt: 100%"}},"01852e9688d142dd8b48709756057d1e":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_61a7680fdda84f6283e0595ec8d2c73f","max":231508,"min":0,"orientation":"horizontal","style":"IPY_MODEL_ad04462e11c24b4eaf1e889fe5ac2053","value":231508}},"b5ae8de97fbd4c7d930baec3b1747931":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_c45468050d6540999a9f71ec83b60a95","placeholder":"​","style":"IPY_MODEL_3d900f8fab9547b09d9fc4da94b76bec","value":" 232k/232k [00:00&lt;00:00, 3.18MB/s]"}},"85b52838d1c54bfeaca883dc95da1466":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"676963610ce349039c32eac82cd0d86a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b39c15429cbb497bb09d0eb2e5101c52":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"61a7680fdda84f6283e0595ec8d2c73f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ad04462e11c24b4eaf1e889fe5ac2053":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"c45468050d6540999a9f71ec83b60a95":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3d900f8fab9547b09d9fc4da94b76bec":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"ee911f76360d4c27b8930ac500a1e1dd":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_c3526ec1fb5344688ac7afbb745bd83b","IPY_MODEL_b9608aa2d33540a5ae7fc1998a334a6b","IPY_MODEL_a9776b81bca84531860f6dc3bda0aaea"],"layout":"IPY_MODEL_44a1ebe0e25a459ab5fe1cb5696f3d48"}},"c3526ec1fb5344688ac7afbb745bd83b":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_603253b6dc8d4e08b9096797653ba708","placeholder":"​","style":"IPY_MODEL_4b0658c5de554c1d80329a048b5d186c","value":"tokenizer.json: 100%"}},"b9608aa2d33540a5ae7fc1998a334a6b":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_d9705c71bdb6464096444139961ae57d","max":466062,"min":0,"orientation":"horizontal","style":"IPY_MODEL_bad0562d54e94b30b9b84bf2a16bace6","value":466062}},"a9776b81bca84531860f6dc3bda0aaea":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_d44bbb8d70e3491cbcd605f60b9f85d2","placeholder":"​","style":"IPY_MODEL_bf66309736f94f6eae0f0d2c97b51b8e","value":" 466k/466k [00:00&lt;00:00, 13.6MB/s]"}},"44a1ebe0e25a459ab5fe1cb5696f3d48":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"603253b6dc8d4e08b9096797653ba708":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4b0658c5de554c1d80329a048b5d186c":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"d9705c71bdb6464096444139961ae57d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"bad0562d54e94b30b9b84bf2a16bace6":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"d44bbb8d70e3491cbcd605f60b9f85d2":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"bf66309736f94f6eae0f0d2c97b51b8e":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"We8SrH1QEFFR","executionInfo":{"status":"ok","timestamp":1711213314746,"user_tz":240,"elapsed":16869,"user":{"displayName":"Blessing Torsu","userId":"13832467145536111066"}},"outputId":"a9baa274-b094-4c40-b3d2-18866ce3543d"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at content\n"]}],"source":["from google.colab import drive\n","drive.mount('content')"]},{"cell_type":"code","source":["!pip install packaging==21.3\n","!pip install transformers==4.5.0"],"metadata":{"id":"H0ukOLnfEH0c"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Import libraries\n","from transformers import AdamW, BertTokenizerFast, BertForQuestionAnswering\n","import math\n","import json\n","import numpy as np\n","import random\n","import torch\n","from torch.utils.data import DataLoader, Dataset\n","from transformers import AdamW, BertForQuestionAnswering, BertTokenizerFast\n","from transformers import AutoModelForQuestionAnswering, AutoTokenizer\n","from transformers import get_linear_schedule_with_warmup\n","\n","from tqdm.auto import tqdm\n","\n","device = torch.device(\"cuda\", 1) if torch.cuda.is_available() else \"cpu\""],"metadata":{"id":"64KRrbkRE4lv","executionInfo":{"status":"ok","timestamp":1711235146870,"user_tz":240,"elapsed":5795,"user":{"displayName":"Blessing Torsu","userId":"13832467145536111066"}}},"execution_count":2,"outputs":[]},{"cell_type":"code","source":["# Change \"fp16_training\" to True to support automatic mixed precision training (fp16)\n","fp16_training = True\n","\n","if fp16_training:\n","    %pip install accelerate==0.2.0\n","    from accelerate import Accelerator\n","    accelerator = Accelerator(fp16=True)\n","    device = accelerator.device"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_WpnIfHhSQs5","executionInfo":{"status":"ok","timestamp":1711235266687,"user_tz":240,"elapsed":118788,"user":{"displayName":"Blessing Torsu","userId":"13832467145536111066"}},"outputId":"2ba820fb-0f83-4ed3-c95b-e63178e633b2"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting accelerate==0.2.0\n","  Downloading accelerate-0.2.0-py3-none-any.whl (47 kB)\n","\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/47.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.3/47.3 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: torch>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from accelerate==0.2.0) (2.2.1+cu121)\n","Collecting pyaml>=20.4.0 (from accelerate==0.2.0)\n","  Downloading pyaml-23.12.0-py3-none-any.whl (23 kB)\n","Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from pyaml>=20.4.0->accelerate==0.2.0) (6.0.1)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.4.0->accelerate==0.2.0) (3.13.1)\n","Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.4.0->accelerate==0.2.0) (4.10.0)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.4.0->accelerate==0.2.0) (1.12)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.4.0->accelerate==0.2.0) (3.2.1)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.4.0->accelerate==0.2.0) (3.1.3)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.4.0->accelerate==0.2.0) (2023.6.0)\n","Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch>=1.4.0->accelerate==0.2.0)\n","  Downloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m36.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting nvidia-cuda-runtime-cu12==12.1.105 (from torch>=1.4.0->accelerate==0.2.0)\n","  Downloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m823.6/823.6 kB\u001b[0m \u001b[31m33.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting nvidia-cuda-cupti-cu12==12.1.105 (from torch>=1.4.0->accelerate==0.2.0)\n","  Downloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m52.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting nvidia-cudnn-cu12==8.9.2.26 (from torch>=1.4.0->accelerate==0.2.0)\n","  Downloading nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m731.7/731.7 MB\u001b[0m \u001b[31m749.6 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting nvidia-cublas-cu12==12.1.3.1 (from torch>=1.4.0->accelerate==0.2.0)\n","  Downloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.6/410.6 MB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting nvidia-cufft-cu12==11.0.2.54 (from torch>=1.4.0->accelerate==0.2.0)\n","  Downloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.6/121.6 MB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting nvidia-curand-cu12==10.3.2.106 (from torch>=1.4.0->accelerate==0.2.0)\n","  Downloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 MB\u001b[0m \u001b[31m11.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting nvidia-cusolver-cu12==11.4.5.107 (from torch>=1.4.0->accelerate==0.2.0)\n","  Downloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.2/124.2 MB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting nvidia-cusparse-cu12==12.1.0.106 (from torch>=1.4.0->accelerate==0.2.0)\n","  Downloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.0/196.0 MB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting nvidia-nccl-cu12==2.19.3 (from torch>=1.4.0->accelerate==0.2.0)\n","  Downloading nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl (166.0 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m166.0/166.0 MB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting nvidia-nvtx-cu12==12.1.105 (from torch>=1.4.0->accelerate==0.2.0)\n","  Downloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m13.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.4.0->accelerate==0.2.0) (2.2.0)\n","Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.4.0->accelerate==0.2.0)\n","  Downloading nvidia_nvjitlink_cu12-12.4.99-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m48.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.4.0->accelerate==0.2.0) (2.1.5)\n","Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.4.0->accelerate==0.2.0) (1.3.0)\n","Installing collected packages: pyaml, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, accelerate\n","Successfully installed accelerate-0.2.0 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.19.3 nvidia-nvjitlink-cu12-12.4.99 nvidia-nvtx-cu12-12.1.105 pyaml-23.12.0\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/torch/cuda/amp/grad_scaler.py:126: UserWarning: torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\n","  warnings.warn(\n"]}]},{"cell_type":"code","source":["##set seed\n","def same_seeds(seed):\n","    torch.manual_seed(seed)\n","    if torch.cuda.is_available():\n","        torch.cuda.manual_seed(seed)\n","        torch.cuda.manual_seed_all(seed)\n","    np.random.seed(seed)\n","    random.seed(seed)\n","    torch.backends.cudnn.benchmark = False\n","    torch.backends.cudnn.deterministic = True\n","\n","same_seeds(0)"],"metadata":{"id":"aUFWsVvhEH3T","executionInfo":{"status":"ok","timestamp":1711235270153,"user_tz":240,"elapsed":177,"user":{"displayName":"Blessing Torsu","userId":"13832467145536111066"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["#Load BERT Model and Tokenizer\n","model = BertForQuestionAnswering.from_pretrained(\"bert-base-uncased\").to(device)\n","tokenizer = BertTokenizerFast.from_pretrained(\"bert-base-uncased\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":168,"referenced_widgets":["30039e9b48ab4e10b0a273ae3c96ba84","491e74054e2b41c6bb399b5f75ddbbc9","cec7477a281d43a4a451fe8df6da120b","f33e26922bd1411ebc215c6653672e81","429ad9a3a0634802b4107eb0fd4c8dfe","25a80cd93e844819920ceb85189fbecb","629362288d9b41d8846ab3f4162914b9","3b2a72e0838f4aa4a9907baa1c96b36a","b45b3c330201474aac99b07cfdc021ea","ec29a4b98bca47a5aecd49a0bb879af2","f68de5c27e5646289fa224c896892b2d","6741d53566be4086ac2ba85dbb979952","fad2e9ceef184cec89de85b70921f838","01852e9688d142dd8b48709756057d1e","b5ae8de97fbd4c7d930baec3b1747931","85b52838d1c54bfeaca883dc95da1466","676963610ce349039c32eac82cd0d86a","b39c15429cbb497bb09d0eb2e5101c52","61a7680fdda84f6283e0595ec8d2c73f","ad04462e11c24b4eaf1e889fe5ac2053","c45468050d6540999a9f71ec83b60a95","3d900f8fab9547b09d9fc4da94b76bec","ee911f76360d4c27b8930ac500a1e1dd","c3526ec1fb5344688ac7afbb745bd83b","b9608aa2d33540a5ae7fc1998a334a6b","a9776b81bca84531860f6dc3bda0aaea","44a1ebe0e25a459ab5fe1cb5696f3d48","603253b6dc8d4e08b9096797653ba708","4b0658c5de554c1d80329a048b5d186c","d9705c71bdb6464096444139961ae57d","bad0562d54e94b30b9b84bf2a16bace6","d44bbb8d70e3491cbcd605f60b9f85d2","bf66309736f94f6eae0f0d2c97b51b8e"]},"id":"W8NAEdz1EICo","executionInfo":{"status":"ok","timestamp":1711214877252,"user_tz":240,"elapsed":1399,"user":{"displayName":"Blessing Torsu","userId":"13832467145536111066"}},"outputId":"f2255279-d2b0-4ad5-d480-8bab8710e1f8"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"output_type":"display_data","data":{"text/plain":["tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"30039e9b48ab4e10b0a273ae3c96ba84"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6741d53566be4086ac2ba85dbb979952"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ee911f76360d4c27b8930ac500a1e1dd"}},"metadata":{}}]},{"cell_type":"code","source":["##Load data\n","def convert_json(input_file, output_file):\n","    # Load the original JSON file\n","    with open(input_file, 'r') as f:\n","        original_data = json.load(f)\n","\n","    # Initialize lists to store questions and paragraphs\n","    questions = []\n","    paragraphs = []\n","\n","    # Extract data from original JSON and restructure it\n","    for data_item in original_data['data']:\n","        for paragraph_data in data_item['paragraphs']:\n","            paragraph_text = paragraph_data['context']\n","            paragraphs.append(paragraph_text)\n","            for qa in paragraph_data['qas']:\n","                question_id = len(questions)\n","                answer_start = qa['answers'][0]['answer_start']\n","                answer_end = answer_start + len(qa['answers'][0]['text'])\n","                question_text = qa['question']\n","                answer_text = qa['answers'][0]['text']\n","                questions.append({\n","                    \"id\": question_id,\n","                    \"paragraph_id\": len(paragraphs) - 1,\n","                    \"question_text\": question_text,\n","                    \"answer_text\": answer_text,\n","                    \"answer_start\": answer_start,\n","                    \"answer_end\": answer_end\n","                })\n","\n","    # Create the new JSON structure\n","    new_json = {\n","        \"questions\": questions,\n","        \"paragraphs\": paragraphs\n","    }\n","\n","    # Save the new JSON to a file\n","    with open(output_file, 'w') as f:\n","        json.dump(new_json, f, indent=4, ensure_ascii=False)\n","\n","# Example usage:\n","convert_json('spoken_train-v1.1.json', 'converted_train.json')\n","convert_json('spoken_test-v1.1_WER54.json', 'converted_dev.json')\n","convert_json('spoken_test-v1.1.json', 'converted_test.json')"],"metadata":{"id":"pqJ36HxyEIFP","executionInfo":{"status":"ok","timestamp":1711235289625,"user_tz":240,"elapsed":4971,"user":{"displayName":"Blessing Torsu","userId":"13832467145536111066"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["##data size\n","def read_data(file):\n","    with open(file, 'r', encoding=\"utf-8\") as reader:\n","        data = json.load(reader)\n","    return data[\"questions\"], data[\"paragraphs\"]\n","\n","train_questions, train_paragraphs = read_data(\"converted_train.json\")\n","dev_questions, dev_paragraphs     = read_data(\"converted_dev.json\")\n","test_questions, test_paragraphs   = read_data(\"converted_test.json\")\n","\n","print('train_questions : ', len(train_questions))\n","print('validation_questions   : ', len(dev_questions))\n","print('test_questions  : ', len(test_questions))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"sDZNZduREIHt","executionInfo":{"status":"ok","timestamp":1711235302857,"user_tz":240,"elapsed":446,"user":{"displayName":"Blessing Torsu","userId":"13832467145536111066"}},"outputId":"7d3c93b8-3012-4c54-d104-ec081a63975a"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["train_questions :  37111\n","validation_questions   :  5351\n","test_questions  :  5351\n"]}]},{"cell_type":"code","source":["# Tokenize questions and paragraphs separately\n","train_questions_tokenized  = tokenizer([train_question[\"question_text\"] for train_question in train_questions], add_special_tokens=False)\n","dev_questions_tokenized    = tokenizer([dev_question[\"question_text\"] for dev_question in dev_questions], add_special_tokens=False)\n","test_questions_tokenized   = tokenizer([test_question[\"question_text\"] for test_question in test_questions], add_special_tokens=False)\n","\n","train_paragraphs_tokenized = tokenizer(train_paragraphs, add_special_tokens=False)\n","dev_paragraphs_tokenized   = tokenizer(dev_paragraphs, add_special_tokens=False)\n","test_paragraphs_tokenized  = tokenizer(test_paragraphs, add_special_tokens=False)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qemvkUfbGe9v","executionInfo":{"status":"ok","timestamp":1711214928689,"user_tz":240,"elapsed":18293,"user":{"displayName":"Blessing Torsu","userId":"13832467145536111066"}},"outputId":"8a4b7374-7c4d-4537-b8dd-c1d100d34168"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["Token indices sequence length is longer than the specified maximum sequence length for this model (605 > 512). Running this sequence through the model will result in indexing errors\n"]}]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Xjooag-Swnuh"},"outputs":[],"source":["DOC_STRIDE = None\n","\n","class QA_Dataset(Dataset):\n","    def __init__(self, split, questions, tokenized_questions, tokenized_paragraphs):\n","        self.split = split\n","        self.questions = questions\n","        self.tokenized_questions = tokenized_questions\n","        self.tokenized_paragraphs = tokenized_paragraphs\n","        self.max_question_len = 40\n","        self.max_paragraph_len = 350\n","\n","        ##### Change value of doc_stride #####\n","        self.doc_stride = int(0.9 * self.max_paragraph_len)\n","\n","        ############################################\n","        global DOC_STRIDE\n","\n","        DOC_STRIDE = self.doc_stride\n","        ############################################\n","        # Input sequence length = [CLS] + question + [SEP] + paragraph + [SEP]\n","\n","        self.max_seq_len = 1 + self.max_question_len + 1 + self.max_paragraph_len + 1\n","\n","    def __len__(self):\n","        return len(self.questions)\n","\n","    def __getitem__(self, idx):\n","        question = self.questions[idx]\n","        tokenized_question = self.tokenized_questions[idx]\n","        tokenized_paragraph = self.tokenized_paragraphs[question[\"paragraph_id\"]]\n","\n","        ##### Preprocessing #####\n","\n","        if self.split == \"train\":\n","            # Convert answer's start/end positions in paragraph_text to start/end positions in tokenized_paragraph\n","            answer_start_token = tokenized_paragraph.char_to_token(question[\"answer_start\"])\n","            answer_end_token = tokenized_paragraph.char_to_token(question[\"answer_end\"])\n","            if answer_start_token is None or answer_end_token is None:\n","              answer_start_token = 0\n","              answer_end_token = 0\n","\n","            # A single window is obtained by slicing the portion of paragraph containing the answer\n","            mid = (answer_start_token + answer_end_token) // 2\n","            prefix_len = int(random.random() * self.max_paragraph_len)\n","            postfix_len = self.max_paragraph_len - prefix_len\n","            paragraph_start, paragraph_end = mid - prefix_len, mid + postfix_len\n","            if paragraph_start < 0:\n","                paragraph_end -= paragraph_start\n","                paragraph_start = 0\n","            if paragraph_end >= len(tokenized_paragraph):\n","                paragraph_end = len(tokenized_paragraph) - 1\n","\n","            # Slice question/paragraph and add special tokens (101: CLS, 102: SEP)\n","            input_ids_question = [101] + tokenized_question.ids[:self.max_question_len] + [102]\n","            input_ids_paragraph = tokenized_paragraph.ids[paragraph_start : paragraph_end] + [102]\n","\n","            # Convert answer's start/end positions in tokenized_paragraph to start/end positions in the window\n","            answer_start_token += len(input_ids_question) - paragraph_start\n","            answer_end_token += len(input_ids_question) - paragraph_start\n","\n","            # Pad sequence and obtain inputs to model\n","            input_ids, token_type_ids, attention_mask = self.padding(input_ids_question, input_ids_paragraph)\n","            return torch.tensor(input_ids), torch.tensor(token_type_ids), torch.tensor(attention_mask), answer_start_token, answer_end_token\n","\n","        # Validation\n","        # Testing\n","        else:\n","            input_ids_list, token_type_ids_list, attention_mask_list = [], [], []\n","\n","            # Paragraph is split into several windows, each with start positions separated by step \"doc_stride\"\n","            for i in range(0, len(tokenized_paragraph), self.doc_stride):\n","\n","                # Slice question/paragraph and add special tokens (101: CLS, 102: SEP)\n","                input_ids_question = [101] + tokenized_question.ids[:self.max_question_len] + [102]\n","                input_ids_paragraph = tokenized_paragraph.ids[i : i + self.max_paragraph_len] + [102]\n","\n","                # Pad sequence and obtain inputs to model\n","                input_ids, token_type_ids, attention_mask = self.padding(input_ids_question, input_ids_paragraph)\n","\n","                input_ids_list.append(input_ids)\n","                token_type_ids_list.append(token_type_ids)\n","                attention_mask_list.append(attention_mask)\n","\n","            return torch.tensor(input_ids_list), torch.tensor(token_type_ids_list), torch.tensor(attention_mask_list)\n","\n","    def padding(self, input_ids_question, input_ids_paragraph):\n","        # Pad zeros if sequence length is shorter than max_seq_len\n","        padding_len = self.max_seq_len - len(input_ids_question) - len(input_ids_paragraph)\n","        # Indices of input sequence tokens in the vocabulary\n","        input_ids = input_ids_question + input_ids_paragraph + [0] * padding_len\n","        # Segment token indices to indicate first and second portions of the inputs. Indices are selected in [0, 1]\n","        token_type_ids = [0] * len(input_ids_question) + [1] * len(input_ids_paragraph) + [0] * padding_len\n","        # Mask to avoid performing attention on padding token indices. Mask values selected in [0, 1]\n","        attention_mask = [1] * (len(input_ids_question) + len(input_ids_paragraph)) + [0] * padding_len\n","\n","        return input_ids, token_type_ids, attention_mask\n","\n","train_set = QA_Dataset(\"train\", train_questions, train_questions_tokenized, train_paragraphs_tokenized)\n","dev_set = QA_Dataset(\"dev\", dev_questions, dev_questions_tokenized, dev_paragraphs_tokenized)\n","test_set = QA_Dataset(\"test\", test_questions, test_questions_tokenized, test_paragraphs_tokenized)\n","\n","train_batch_size = 4\n","\n","#########################\n","\n","train_loader = DataLoader(train_set, batch_size=train_batch_size, shuffle=True, pin_memory=True)\n","dev_loader   = DataLoader(dev_set, batch_size=1, shuffle=False, pin_memory=True)\n","test_loader  = DataLoader(test_set, batch_size=1, shuffle=False, pin_memory=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SqeA3PLPxOHu"},"outputs":[],"source":["# Defime evaluation function\n","def evaluate(data, output, paragraph, paragraph_tokenized):\n","    ##### Postprocessing #####\n","\n","    answer = ''\n","    max_prob = float('-inf')\n","    num_of_windows = data[0].shape[1]\n","\n","    paragraph_start_index = 0\n","    paragraph_end_index = 0\n","\n","    for k in range(num_of_windows):\n","\n","        mask = (data[1][0][k].bool() & data[2][0][k].bool()).to(device)\n","\n","        masked_output_start = torch.masked_select(output.start_logits[k], mask)\n","        masked_output_start = masked_output_start[:-1]\n","\n","        start_prob, start_index = torch.max(masked_output_start, dim=0)\n","\n","        masked_output_end = torch.masked_select(output.end_logits[k], mask)\n","        masked_output_end = masked_output_end[start_index: -1]\n","\n","        end_prob, end_index = torch.max(masked_output_end, dim=0)\n","\n","        end_index += start_index\n","\n","        # Probability of answer is calculated as sum of start_prob and end_prob\n","        prob = start_prob + end_prob\n","        masked_data = torch.masked_select(data[0][0][k].to(device), mask)[:-1]\n","\n","        # Replace answer if calculated probability is larger than previous windows\n","        if (prob > max_prob) and (start_index <= end_index <= (start_index + 50)):\n","            max_prob = prob\n","            paragraph_start_index = start_index.item() + (DOC_STRIDE * k)\n","            paragraph_end_index = end_index.item() + (DOC_STRIDE * k)\n","            answer = tokenizer.decode(masked_data[start_index : end_index + 1])\n","\n","\n","\n","    ##########\n","    char_count = 0\n","    start_flag = False\n","\n","    for i, token in enumerate(paragraph_tokenized):\n","        if token in ('[UNK]', '[CLS]', '[SEP]'):\n","            if i == paragraph_start_index:\n","                new_start = char_count\n","            if i == paragraph_end_index:\n","                new_end = char_count\n","            char_count += 1\n","        else:\n","            for char in token:\n","                if i == paragraph_start_index and not start_flag:\n","                    new_start = char_count\n","                    start_flag = True\n","                if i == paragraph_end_index:\n","                    new_end = char_count\n","                if char == \"#\":\n","                    continue\n","                else:\n","                    while char_count < len(paragraph) and char != paragraph[char_count]:\n","                        char_count += 1\n","                    char_count += 1\n","\n","    if \"[UNK]\" in answer:\n","        print(f\"original answer: {answer}\")\n","        answer = paragraph[new_start: new_end+1]\n","        print(f\"corrected answer: {answer}\")\n","        print(\"-\"*50)\n","\n","\n","    answer = answer.replace(' ', '')\n","\n","\n","    if len(answer) > 1:\n","        if \"「\" not in answer and answer[-1] == \"」\":\n","            answer = answer[:-1]\n","    return answer"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3Q-B6ka7xoCM"},"outputs":[],"source":["# Training process\n","num_epoch     =  3\n","validation    = True\n","logging_step  = 600\n","learning_rate = 1e-3\n","\n","optimizer = AdamW(model.parameters(), lr=learning_rate)\n","\n","##### Apply linear learning rate decay #####\n","total_steps = len(train_loader) * num_epoch\n","\n","\n","scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=total_steps)\n","\n","##################################################\n","\n","if fp16_training:\n","    model, optimizer, train_loader = accelerator.prepare(model, optimizer, train_loader)\n","\n","model.train()\n","\n","print(\"Start Training ...\")\n","\n","for epoch in range(num_epoch):\n","    step = 1\n","    train_loss = train_acc = 0\n","\n","    for batch_idx, data in enumerate(tqdm(train_loader)):\n","        # Load all data into GPU\n","        data = [i.to(device) for i in data]\n","\n","        # Model inputs: input_ids, token_type_ids, attention_mask, start_positions, end_positions (Note: only \"input_ids\" is mandatory)\n","        # Model outputs: start_logits, end_logits, loss (return when start_positions/end_positions are provided)\n","        output = model(input_ids=data[0], token_type_ids=data[1], attention_mask=data[2], start_positions=data[3], end_positions=data[4])\n","\n","        # Choose the most probable start position / end position\n","        start_index = torch.argmax(output.start_logits, dim=1)\n","        end_index = torch.argmax(output.end_logits, dim=1)\n","\n","        # Prediction is correct only if both start_index and end_index are correct\n","        train_acc += ((start_index == data[3]) & (end_index == data[4])).float().mean()\n","        train_loss += output.loss\n","\n","        if fp16_training:\n","            accelerator.backward(output.loss)\n","        else:\n","            output.loss.backward()\n","\n","        ##### Apply linear learning rate decay #####\n","        optimizer.step()\n","        scheduler.step()\n","        optimizer.zero_grad()\n","        step += 1\n","        ##################################################\n","\n","        # Print training loss and accuracy over past logging step\n","        if step % logging_step == 0:\n","            print(f\"Epoch {epoch + 1} | Step {step} | loss = {train_loss.item() / logging_step:.3f}, acc = {train_acc / logging_step:.3f}\")\n","            train_loss = train_acc = 0\n","\n","    if validation:\n","        print(\"Evaluating Dev Set ...\")\n","        model.eval()\n","        with torch.no_grad():\n","            dev_acc = 0\n","            for i, data in enumerate(tqdm(dev_loader)):\n","                output = model(input_ids=data[0].squeeze(dim=0).to(device), token_type_ids=data[1].squeeze(dim=0).to(device),\n","                       attention_mask=data[2].squeeze(dim=0).to(device))\n","                # prediction is correct only if answer text exactly matches\n","                dev_acc += evaluate(data, output, dev_paragraphs[dev_questions[i]['paragraph_id']], dev_paragraphs_tokenized[dev_questions[i]['paragraph_id']].tokens) == dev_questions[i][\"answer_text\"]\n","            print(f\"Validation | Epoch {epoch + 1} | acc = {dev_acc / len(dev_loader):.3f}\")\n","        model.train()\n","\n","\n","##################################################\n","\n","# Save a model\n","print(\"Saving Model ...\")\n","model_save_dir = \"./models/saved_model\"\n","model.save_pretrained(model_save_dir)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"N_l99uR-9Gvx"},"outputs":[],"source":["# Model Validation\n","model.eval()\n","with torch.no_grad():\n","    dev_acc = 0\n","    for i, data in enumerate(tqdm(dev_loader)):\n","        output = model(input_ids=data[0].squeeze(dim=0).to(device), token_type_ids=data[1].squeeze(dim=0).to(device),\n","               attention_mask=data[2].squeeze(dim=0).to(device))\n","        # prediction is correct only if answer text exactly matches\n","        pred_answer = evaluate(data, output, dev_paragraphs[dev_questions[i]['paragraph_id']], dev_paragraphs_tokenized[dev_questions[i]['paragraph_id']].tokens)\n","        true_answer = dev_questions[i][\"answer_text\"]\n","        dev_acc += (pred_answer == true_answer)\n","        if pred_answer != true_answer:\n","            print(\"*\"*50)\n","            print(f\"correct answer: {true_answer}\")\n","            print(f\"predict answer: {pred_answer}\")\n","            print(\"*\"*50)\n","    print(f\"Validation | acc = {dev_acc / len(dev_loader):.3f}\")\n","model.train()\n","# print(f\"Validation | accuracy = {dev_acc / len(dev_loader):.3f}\")"]},{"cell_type":"code","source":["print(f\"Validation | accuracy = {dev_acc / len(dev_loader):.3f}\")"],"metadata":{"id":"L2CP-BmRQI1X","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1711241748406,"user_tz":240,"elapsed":123,"user":{"displayName":"Blessing Torsu","userId":"13832467145536111066"}},"outputId":"97a65e7b-18b9-4449-90a1-a3c41865388e"},"execution_count":24,"outputs":[{"output_type":"stream","name":"stdout","text":["Validation | accuracy =  0.737\n"]}]},{"cell_type":"code","execution_count":null,"metadata":{"id":"U5scNKC9xz0C"},"outputs":[],"source":["\n","# Test model\n","print(\"Evaluating Test Set ...\")\n","\n","result = []\n","\n","model.eval()\n","with torch.no_grad():\n","    for i, data in enumerate(tqdm(test_loader)):\n","        output = model(input_ids=data[0].squeeze(dim=0).to(device), token_type_ids=data[1].squeeze(dim=0).to(device),\n","                       attention_mask=data[2].squeeze(dim=0).to(device))\n","        result.append(evaluate(data, output, test_paragraphs[test_questions[i]['paragraph_id']], test_paragraphs_tokenized[test_questions[i]['paragraph_id']].tokens))\n","\n","result_file = \"./result.csv\"\n","with open(result_file, 'w') as f:\n","    f.write(\"ID,Answer\\n\")\n","    for i, test_question in enumerate(test_questions):\n","        # Replace commas in answers with empty strings (since csv is separated by comma)\n","        f.write(f\"{test_question['id']},{result[i].replace(',','')}\\n\")\n","\n","print(f\"Completed! Result is in {result_file}\")"]}]}